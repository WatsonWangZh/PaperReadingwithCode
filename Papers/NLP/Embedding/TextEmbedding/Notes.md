# Text Embedding
## word2vec(2013):
* 2003年提出的NNLM和2010年提出的RNNLM模型，都是以语言建模的方式学习词向量，理论基础是建模句子中词的联合概率分布，将语料中片段的频率作为概率。由于假设过弱，语言模型会面临OOV和参数空间过大的问题。对于OOV的问题可以用平滑方法部分缓解。而对于参数空间过大的问题，NNLM采用了马尔可夫假设。
* word2vec创新性地提出了以词预测词的方法，来建模语言。包括两种模型cbow和skip-gram模型，一般情况下skip-gram模型要优于cbow模型。