# Paper Reading with Code
A Personal Library.  Some paper I have read and to read, with notes and code.

# Backbone
## NLP
* origin LSTM term Neural Computation 1997 [LONG SHORT-TERM MEMORY](https://www.bioinf.jku.at/publications/older/2604.pdf) [code]() 
## CV
* NNLM JMLR 2003 [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) [code]() 
  
# Embedding
## [Text Embedding]()
### Word Embedding
* NNLM JMLR 2003 [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) [code]()  
* RNNLM InterSpeech 2010 [Recurrent neural network based language model](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) [code]()  
* word2vec Architecture ICLR 2013 [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) [code]()  
* word2vec Tricks NIPS 2013 [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) [code]()
* GloVe EMNLP 2014 [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162.pdf) [code]()  
* char2wordvec EMNLP 2015 [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/pdf/1508.02096.pdf) [code]()   
## Sentence Embedding
* RAE NIPS 2011 [Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection](https://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf) [code]()  
* EMNLP 2012 [Semantic Compositionality through Recursive Matrix-Vector Spaces](https://ai.stanford.edu/~ang/papers/emnlp12-SemanticCompositionalityRecursiveMatrixVectorSpaces.pdf) [code]() [resources]() 
* ACL 2014 [A Convolutional Neural Network for Modelling Sentences](http://mirror.aclweb.org/acl2014/P14-1/pdf/P14-1062.pdf) [code]() 
* Skip-Thought NIPS 2015 [Skip-Thought Vectors](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf) [code]()
* Quick-Thought ICLR 2018 [AN EFFICIENT FRAMEWORK FOR LEARNING SENTENCE REPRESENTATIONS](https://arxiv.org/pdf/1803.02893.pdf) [code]()   
## Context Embedding
* CoNLL 2016 [context2vec: Learning Generic Context Embedding with Bidirectional LSTM](https://www.aclweb.org/anthology/K16-1006.pdf) [code]()  
## Paragraph Embedding
* PMLR 2014 [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053.pdf) [code]()  
## [Graph Embedding]()
* TransE NIPS 2013 [Translating Embeddings for Modeling Multi-relational Data](https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf) [code]()  
* DeepWalk KDD 2014 [DeepWalk: Online Learning of Social Representations](https://arxiv.org/pdf/1403.6652.pdf) [code]()  
* LINE WWW 2015[LINE: Large-scale Information Network Embedding](https://arxiv.org/pdf/1503.03578.pdf) [code]() 
* node2vec KDD 2016[node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) [code]()   


# [Pre-trained Language Model]()
* NNLM JMLR 2003 [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) [code]()  
* RNNLM InterSpeech 2010 [Recurrent neural network based language model](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) [code]()   
* ELMO NAACL 2018 [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf) 
* GPT 1.0 InterSpeech 2018 [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) [code]()  
* BERT InterSpeech 2018 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) [code]() 
* Transformer-XL ACL 2019 [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf) [code]()  
* GPT 2.0 OpenAI Report 2019 [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) [Slides](https://pdfs.semanticscholar.org/41f9/45f59bd0d345d4e355fb72110524f6fdffdb.pdf) [code]()  
* ERNIE (Tsinghua) ACL 2019 [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/pdf/1905.07129.pdf) [code]()  
* ERNIE 1.0 arXiv 2019 [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223.pdf) [code]()  
* ERNIE 2.0 AAAI 2020 [ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding](https://arxiv.org/pdf/1907.12412.pdf) [code]()  
* MASS ICML 2019 [MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/pdf/1905.02450.pdf) [code]() 
* UniLM arXiv 2019 [Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/pdf/1905.03197.pdf) [code]()  
* XLNet NIPS 2019 [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf) [code]()  
* ALBERT ICLR 2020 [ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://arxiv.org/pdf/1909.11942.pdf) [code](https://github.com/google-research/ALBERT)  

# [Seq2Seq]()
* EMNLP 2014 [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://www.aclweb.org/anthology/D14-1179) [code]() 
* NIPS 2014 [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) [code]() 
* EMNLP 2014 [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) [code]() 
* ByteNet arXiv 2017 [Neural Machine Translation in Linear Time](https://arxiv.org/pdf/1610.10099v1.pdf) [Slides](http://llcao.net/cu-deeplearning17/pp/class8_TranslationinLinearTime.pdf) [code]() 
* Transformer NIPS 2017 [Attention Is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) [code]() 

# [Sequence Labeling]()
* BiLSTM-CRF arXiv 2015 [Bidirectional LSTM-CRF Models for Sequence Tagging](https://arxiv.org/pdf/1508.01991.pdf) [code]() 
* CNN-BiLSTM-CRF arXiv 2016 [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF](https://arxiv.org/pdf/1603.01354.pdf) [code]() 
* Lattice LSTM ACL 2018 [Chinese NER Using Lattice LSTM](https://www.aclweb.org/anthology/P18-1144.pdf)[code]() 

# Text Classification
* ACL 2014 [Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181.pdf)[code]() 
* NIPS 2015 [Character-level Convolutional Networks for Text Classification](https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf)[code]() 
* HAN ACL 2016[Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)[code]() 
* FastText EACL 2017 [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)[code]() 


# [Graph Neural Network]()
## Survey
* arXiv 2018[Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/pdf/1812.08434.pdf)  [code]() 
## Spatial Domain
* GNN arXiv 2017 [A Generalization of Convolutional Neural Networks to Graph-Structured Data](https://arxiv.org/pdf/1704.08165.pdf)  [code]() 
* GraphSAGE NIPS 2017 [Inductive Representation Learning on Large Graphs](https://arxiv.org/pdf/1706.02216.pdf)  [code]()
* GAT ICLR 2018 [GRAPH ATTENTION NETWORKS](https://arxiv.org/pdf/1710.10903.pdf)  [code]() 
* PGC AAAI 2018 [Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/pdf/1801.07455.pdf)  [code]() 
## Spectral Domain
* SCNN ICLR 2014 [Spectral Networks and Deep Locally Connected Networks on Graphs](https://arxiv.org/pdf/1312.6203.pdf)  [code]()  
* ChebNet NIPS 2016 [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/pdf/1606.09375.pdf)  [code]() 
* GCN [Spectral Networks and Deep Locally Connected Networks on Graphs](https://arxiv.org/pdf/1609.02907.pdf)  [code]() 

# Practical Techique
* Dropout JMLR 2014 [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)  [code]()
* Mixout ICLR 2020 [MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE LARGE-SCALE PRETRAINED LANGUAGE MODELS](https://arxiv.org/pdf/1909.11299.pdf)  [code]()
* Disout AAAI 2020 [Beyond Dropout: Feature Map Distortion to Regularize Deep Neural Networks](https://arxiv.org/pdf/2002.11022.pdf)  [code]()
* Gradient Clipping Explanation ICLR 2020 [WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY](https://arxiv.org/pdf/1905.11881.pdf)
* Batch Normalization ICML 2015 [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)  [code]()
* Batch Normalization Explanation NIPS 2018 [How Does Batch Normalization Help Optimization?](http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf)  [code]()
* Layer Normalization arXiv 2016 [Layer Normalization](https://arxiv.org/pdf/1607.06450v1.pdf)  [code]()
* Instance Normalization arXiv 2016 [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/pdf/1607.08022.pdf)  [code]()
* Group Normalization arXiv 2018 [Group Normalization](https://arxiv.org/pdf/1803.08494.pdf)  [code]()

# Ensemble Learning


# Uncategorized

# Resources
* [常用的 Normalization 方法：BN、LN、IN、GN](https://www.chainnews.com/articles/678463364097.htm)